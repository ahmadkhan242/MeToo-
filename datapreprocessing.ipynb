{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary files\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset into dataframeTrain and dataframeTest\n",
    "#To preserve user identity, the datasets uploaded contain only Tweet IDs, Tweet Text, Tweet Hashtags and attached URLs\n",
    "dataframeTrain = pd.read_csv('dataset/train.csv')\n",
    "dataframeTest = pd.read_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing missing values in the train set\n",
    "#uncomment to view\n",
    "#sns.heatmap(dataframeTrain.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing missing values in the test set\n",
    "#uncomment to view\n",
    "#sns.heatmap(dataframeTest.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill NA in text columns with blank space in columns: text, hashtags, urls\n",
    "def fillNA(dataset):\n",
    "    values = {'text':' ', 'hashtags': ' ', 'urls':' '}\n",
    "    dataset = dataset.fillna(value=values)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the function to fill NA values\n",
    "dataframeTrain = fillNA(dataframeTrain)\n",
    "dataframeTest = fillNA(dataframeTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we don't have any missing fields, let us clean the strings individually\n",
    "def datasetCleaner(dataset):\n",
    "    #removes http format from texts\n",
    "    dataset.text = dataset.text.str.replace(r'http(\\S)+', r'')\n",
    "    dataset.hashtags = dataset.hashtags.str.replace(r'http ...', r'')\n",
    "    dataset.urls = dataset.urls.replace(r'http(\\S)+', r'')\n",
    "    #removes RT (retweet)\n",
    "    dataset.text = dataset.text.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "    dataset.hashtags = dataset.hashtags.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "    dataset.urls = dataset.urls.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "    #removes @username\n",
    "    dataset.text = dataset.text.str.replace(r'@[\\S]+',r'')\n",
    "    dataset.hashtags = dataset.hashtags.str.replace(r'@[\\S]+',r'')\n",
    "    dataset.urls = dataset.urls.str.replace(r'@[\\S]+',r'')\n",
    "    #replaces double or multiple spaces with single space\n",
    "    dataset.text = dataset.text.str.replace(r'[ ]{2, }',r' ')\n",
    "    dataset.hashtags = dataset.hashtags.str.replace(r'[ ]{2, }',r' ')\n",
    "    dataset.urls = dataset.urls.str.replace(r'[ ]{2, }',r' ')\n",
    "    #replaces & with and\n",
    "    dataset.text = dataset.text.str.replace(r'&amp;?',r'and')\n",
    "    dataset.hashtags = dataset.hashtags.str.replace(r'&amp;?',r'and')\n",
    "    dataset.urls = dataset.urls.str.replace(r'&amp;?',r'and')\n",
    "    #replaces the text format of symbols with symbols: <,>\n",
    "    dataset.text = dataset.text.str.replace(r'&lt;',r'<')\n",
    "    dataset.text = dataset.text.str.replace(r'&gt;',r'>')\n",
    "    dataset.hashtags = dataset.hashtags.str.replace(r'&lt;',r'<')\n",
    "    dataset.hashtags = dataset.hashtags.str.replace(r'&gt;',r'>')\n",
    "    dataset.urls = dataset.urls.str.replace(r'&lt;',r'<')\n",
    "    dataset.urls = dataset.urls.str.replace(r'&gt;',r'>')\n",
    "    #as most of the encoders and vocabulary embeddings are lower cased, we need to lower case our data\n",
    "    dataset.text = dataset.text.str.lower()\n",
    "    dataset.hashtags = dataset.hashtags.str.lower()\n",
    "    dataset.urls = dataset.urls.str.lower()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the function to clean the datasets\n",
    "dataframeTrain = datasetCleaner(dataframeTrain)\n",
    "dataframeTest = datasetCleaner(dataframeTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer contents of the cleaned text columns into a single text column preprocessedText\n",
    "def createProcessedText(dataset):\n",
    "    processedText = dataset['text'].astype(str)+ ' ' +dataset['hashtags'].astype(str)+ ' ' +dataset['urls'].astype(str)\n",
    "    dataset['processedText'] = processedText\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the function to create processedText column\n",
    "dataframeTrain = createProcessedText(dataframeTrain)\n",
    "dataframeTest = createProcessedText(dataframeTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function that encapsulates the above tasks and can be directly called in training files\n",
    "def preProcessDataset(datasetTrain, datasetTest):\n",
    "    datasetTrain = fillNA(datasetTrain)\n",
    "    datasetTrain = datasetCleaner(datasetTrain)\n",
    "    datasetTrain = createProcessedText(datasetTrain)\n",
    "    datasetTest = fillNA(datasetTest)\n",
    "    datasetTest = datasetCleaner(datasetTest)\n",
    "    datasetTest = createProcessedText(datasetTest)\n",
    "    return datasetTrain, datasetTest  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
